def step_delete(layer,id_list,globalids=True):
    if globalids:
        id_field = layer.properties.globalIdField
        oid_list = id_list
    else:
        id_field = layer.properties.objectIdField
        oid_list = [str(oid) for oid in id_list]
    if len(id_list)>1000:
        start = 0
        step = 1000
        print("Deleting "+str(len(oid_list))+" 1000 records at a time")
        while start<len(id_list):
            print(start,start+step)
            del_query = id_field+""" IN ('"""+"""','""".join(oid_list[start:start+step])+"""')"""
            layer.delete_features(where=del_query)
            start+=step
    else:
        print("Deleting "+str(len(oid_list))+" records")
        del_query = id_field+""" IN ('"""+"""','""".join(oid_list)+"""')"""
        layer.delete_features(where=del_query)
        
    

def step_update(dataset,layer,step_number=2000):
    start=0
    results=[]
    print(layer.properties.name,"- Updates")
    while start< len(dataset):
        print(start,start+step_number)
        results+=(layer.edit_features(updates=dataset[start:start+step_number],use_global_ids=True,rollback_on_failure=False)['updateResults'])
        start+=step_number
    return results
        
def stepload(dataset,destlayer,stepnumber):
    #This tool incrementally adds features to a layer
    #This is helpful for loading data into an SDE-based layer for sync
    #dataset = list of JSON features / dictionaries usually generated by a replica creation
    #destlayer = a FeatureLayer object that you want to load the dataset into
    #stepnumber = an increment integer - how many features you want to add at a time
    ###for points: 5000
    ###for polygons: 1000
    m=0
    total = len(dataset)
    print('Loading Features...')
    while m<total:
        print(m,m+stepnumber)
        oldcount = destlayer.query(return_count_only=True)
        destlayer.edit_features(adds=dataset[m:m+stepnumber],use_global_ids=True)
        currentcount= destlayer.query(return_count_only=True)
        if currentcount==oldcount+stepnumber:
            print('\tSuccessful')
        elif currentcount==total:
            print('ALL DONE')
            break
        else:
            print('\tCurrent Count: '+str(currentcount))
            break
        m+=stepnumber

        
def data_dump_month(flc,data,top_folder):
    '''Sub Function for ReplicaSYNC - writes sync data to text file.
    Will be useful for getting edits in case of errors
        -FLC: Feature Layer Collection - Used only for naming file
        -data: JSON data returned by SYNC
        -topfolder: where the data is to be housed
    This function will make a Year-Month folder structure then make a subfolder
      for each different FLC that is sync'd'''
    current_month = time.strftime("%Y_%m")
    name = flc.url.split('/')[-2]
    month_folder = os.path.join(top_folder,current_month)
    if not os.path.exists(month_folder):
        os.mkdir(month_folder)
    sub_folder = os.path.join(top_folder,current_month,name)
    if not os.path.exists(sub_folder):
        os.mkdir(sub_folder)
    data_txt = os.path.join(sub_folder,name+time.strftime("_%Y%m%d_%H%M%S.txt"))
    with open(data_txt,'w') as data_filer:
        data_filer.write(json.dumps(data))

def replica_sync(flc,replica_id,dump_folder):
    """for sync of specific replica
    need to have a FeatureLayerCollection object and a replica ID
    always load this into a variable - for example:
    MyUpdates = replicaSync(FeatureLayerCollection,'l123lk2j45lk3')"""
    server_gens = flc.replicas.get(replica_id)['layerServerGens']
    json_updates = flc.replicas.synchronize(replica_id = replica_id,
                    transport_type='esriTransportTypeEmbedded', 
                    replica_server_gen = None, 
                    return_ids_for_adds = False,
                    edits = None,
                    return_attachment_databy_url = False,
                    asynchronous = False, 
                    sync_direction = 'download',
                    sync_layers=server_gens,
                    edits_upload_id=None, 
                    edits_upload_format=None,  
                    data_format='json',
                    rollback_on_failure=True)
    data_dump_month(flc,json_updates,dump_folder)
    return json_updates



def applyUpdates(syncJSONedits,destlayer,loglist,updatesOnly=False):
    
    '''by layer: needs to take sync object and go into index
    syncJSONedits needs to be UPDjson['edits'][INDEX]['features']'''
    JSONupdates = syncJSONedits['updates']
    for feature in JSONupdates:
        feature['attributes']['GlobalID']='{'+feature['attributes']['GlobalID']+'}'
    if not updatesOnly:
        JSONadds = syncJSONedits['adds']
        
        JSONdeletes = []
        for glob in syncJSONedits['deleteIds']:
                JSONdeletes.append('{'+glob+'}')
        JSONdeletestring= """['"""+"""','""".join(JSONdeletes)+"""']"""
        
        print("====="+destlayer.properties.name+"=====")
        print("\tAdds: "+str(len(JSONadds)))
        print("\tUpdates: "+str(len(JSONupdates)))
        print("\tDeletes: "+str(len(JSONdeletes)))
        try:
            results  = destlayer.edit_features(adds=JSONadds,updates=JSONupdates,deletes=JSONdeletestring,use_global_ids=True,rollback_on_failure=False)
            loglist.append([destlayer.properties.name,time.strftime("%m/%d/%Y %H:%M"),len(JSONadds),len(JSONupdates),len(JSONdeletes),True,None])
        except:
            print('error - trying to step through process')
            stepload(JSONadds,destlayer,2000)
            step_update(JSONupdates,destlayer,2000)
            step_delete(destlayer,JSONdeletes)
        
    else:
        print("=======================================")
        print("            UPDATE ONLY")
        print("====="+destlayer.properties.name+"=====")
        print("\tAdds: N/A")
        print("\tUpdates: "+str(len(JSONupdates)))
        print("\tDeletes: N/A")       
        results = destlayer.edit_features(updates=JSONupdates,use_global_ids=True,rollback_on_failure=False)
        loglist.append([destlayer.properties.name,time.strftime("%m/%d/%Y %H:%M"),0,len(JSONupdates),0,True,None])
    return results
